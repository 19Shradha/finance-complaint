{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = \"/home/jovyan/work/finance_complaint/finance_artifact/data_preprocessing/20220906_130727/complaint_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finance_complaint.entity.spark_manager import spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://34c0a6083f46:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>finance_complaint</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f57e5c95358>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.read.parquet(data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_HOT_FEATURES = ['company_response','consumer_consent_provided','submitted_via','timely']\n",
    "\n",
    "NUMERICAL_FEATURE = ['diff_in_days']\n",
    "\n",
    "\n",
    "FREQUENCY_ENCODER = ['company','issue','product','state','zip_code']\n",
    "\n",
    "#TOEKNIZER = ['complaint_what_happened']\n",
    "\n",
    "\n",
    "TARGET_FEATURE = ['consumer_disputed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCols=ONE_HOT_FEATURES,outputCols=[f\"idx_{feature}\" for feature in ONE_HOT_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_category(dataframe:DataFrame,columns:List[str]):\n",
    "    top_category=dict()\n",
    "    for column in columns:\n",
    "        top_cat = df.groupBy(column).count().sort(desc('count')).take(1)[0]\n",
    "        print(top_cat)\n",
    "        category = df.groupBy(column).count().sort(desc('count')).take(1)[0][column]\n",
    "        \n",
    "        top_category[column]=category\n",
    "\n",
    "    return top_category\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(company_response='Closed with explanation', count=580728)\n",
      "Row(consumer_consent_provided='N/A', count=470178)\n",
      "Row(submitted_via='Web', count=524238)\n",
      "Row(timely='Yes', count=748267)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'company_response': 'Closed with explanation',\n",
       " 'consumer_consent_provided': 'N/A',\n",
       " 'submitted_via': 'Web',\n",
       " 'timely': 'Yes'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_category(df,ONE_HOT_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5660"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(col('zip_code').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    setattr(df,col,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(company_response='Closed with explanation', count=580728)\n",
      "Row(consumer_consent_provided='N/A', count=470178)\n",
      "Row(submitted_via='Web', count=524238)\n",
      "Row(timely='Yes', count=748267)\n"
     ]
    }
   ],
   "source": [
    "df = df.na.fill(get_top_category(df,ONE_HOT_FEATURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['company',\n",
       " 'company_response',\n",
       " 'complaint_what_happened',\n",
       " 'consumer_consent_provided',\n",
       " 'consumer_disputed',\n",
       " 'issue',\n",
       " 'product',\n",
       " 'state',\n",
       " 'submitted_via',\n",
       " 'timely',\n",
       " 'zip_code',\n",
       " 'diff_in_days']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1628/1362042270.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff_in_days\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNotNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"diff_in_days\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    ".agg({\"diff_in_days\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- company: string (nullable = true)\n",
      " |-- company_response: string (nullable = true)\n",
      " |-- complaint_what_happened: string (nullable = true)\n",
      " |-- consumer_consent_provided: string (nullable = true)\n",
      " |-- consumer_disputed: string (nullable = true)\n",
      " |-- issue: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- submitted_via: string (nullable = true)\n",
      " |-- timely: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- diff_in_days: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "769598"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(f\"{df.diff_in_days} is  null\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|zip_code|count|\n",
      "+--------+-----+\n",
      "|   48382|  944|\n",
      "|   33173|  691|\n",
      "|   33071|  688|\n",
      "|   30349|  598|\n",
      "|   30331|  549|\n",
      "|   30058|  528|\n",
      "|   11375|  506|\n",
      "|   92101|  500|\n",
      "|   20744|  485|\n",
      "|   20774|  476|\n",
      "|   20002|  467|\n",
      "|   90046|  459|\n",
      "|   28269|  451|\n",
      "|   33025|  442|\n",
      "|   30043|  441|\n",
      "|   30281|  436|\n",
      "|   30039|  433|\n",
      "|   32256|  432|\n",
      "|   30294|  428|\n",
      "|   75052|  418|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(['zip_code']).count().filter('zip_code is not null').sort(desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    " \n",
    "# Produces the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1628/3775288336.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Apply the UDF to our Dataset and create a resultant column called `appended_text`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"appended_text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    " \n",
    "# Define our transformation\n",
    "def append_string(s, append_val=\"\"):\n",
    "    \"\"\"\n",
    "    If we see the word `spark` in s, append a string to the current string.\n",
    "    \"\"\"\n",
    "    if s and 'spark' in s:\n",
    "        return s + append_val\n",
    "    return s\n",
    " \n",
    " \n",
    "# Wrap the transformation as a UDF\n",
    "append_udf = udf(lambda row: append_string(row, \" hadoop\"), StringType())\n",
    " \n",
    "# Apply the UDF to our Dataset and create a resultant column called `appended_text`\n",
    "df.withColumn(col(\"appended_text\"), append_udf(col(\"text\"))).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import append_string  # this is the function we wrote above\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import keyword_only \n",
    " # Note: use pyspark.ml.util.keyword_only \n",
    " # if Spark < 2.0 from pyspark.ml import Transformer \n",
    " # #from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, \n",
    " # Params, TypeConverters from pyspark.ml.util import DefaultParamsReadable\n",
    " # , DefaultParamsWritable class StringAppender(Transformer,\n",
    " #  # Base class HasInputCol, # Sets up an inputCol\n",
    " #  parameter HasOutputCol, # Sets up an outputCol parameter DefaultParamsReadable,\n",
    " #  # Makes parameters readable from file DefaultParamsWritable \n",
    " # # Makes parameters writable from file ): \n",
    " # \"\"\" Custom Transformer wrapper class for append_string() \"\"\" \n",
    " # # append_str is a value which we would like to be able to store state for,\n",
    " #  so we create a parameter. append_str = Param( Params._dummy(), \"append_str\", \n",
    " # \"Value we want to append with\", typeConverter=TypeConverters.toString, # This \n",
    " # will allow code to automatically try to convert to string )\n",
    " #  @keyword_only def __init__(self, inputCol=None, outputCol=None, append_str=None): \n",
    " # \"\"\" Constructor: set values for all Param objects \"\"\" \n",
    " # super().__init__() self._setDefault(append_str=None)\n",
    " #  kwargs = self._input_kwargs self.setParams(**kwargs)\n",
    " #  @keyword_only def setParams(self, inputCol=None, outputCol=None, append_str=None):\n",
    " #  kwargs = self._input_kwargs return self._set(**kwargs)\n",
    " #  def setAppendStr(self, new_append_str):\n",
    " #  return self.setParams(append_str=new_append_str) # Required if you use Spark >= 3.0\n",
    "    def setInputCol(self, new_inputCol):\n",
    "        return self.setParams(inputCol=new_inputCol)\n",
    "  \n",
    "    # Required if you use Spark >= 3.0\n",
    "    def setOutputCol(self, new_outputCol):\n",
    "        return self.setParams(outputCol=new_outputCol)\n",
    "  \n",
    "    def getAppendStr(self):\n",
    "        return self.getOrDefault(self.append_str)\n",
    "  \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"\n",
    "        This is the main member function which applies the transform to transform data from the `inputCol` to the `outputCol`\n",
    "        \"\"\"\n",
    "        if not self.isSet(\"inputCol\"):\n",
    "            raise ValueError(\n",
    "                \"No input column set for the \"\n",
    "                \"StringAppenderTransformer transformer.\"\n",
    "            )\n",
    "        input_column = dataset[self.getInputCol()]\n",
    "        output_column = self.getOutputCol()\n",
    "        append_str = self.getAppendStr()\n",
    "        udf_func = lambda x: append_string(x, append_str)\n",
    "        data_type = StringType()\n",
    "         \n",
    "        return dataset.withColumn(output_column,\n",
    "                                  udf(udf_func, data_type)(input_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.base import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler(Transformer):\n",
    "\n",
    "    def __init__(self,*args,**kwargs) -> None:\n",
    "        super().__init__(kwargs)\n",
    "        \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fa0b0c3e0c129e18009469228710308a4c80fb2429a87704e5ca44dc3eb75d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
